{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Press 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_46076/55373931.py:21: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pymysql\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Maria\n",
    "MARIA_USER = os.getenv(\"DB_USER\")\n",
    "MARIA_PASSWORD = os.getenv(\"DB_PASS\")\n",
    "MARIA_IP = os.getenv(\"DB_IP\")\n",
    "MARIA_PORT = os.getenv(\"DB_PORT\")\n",
    "MARIA_NAME = os.getenv(\"DB_NAME\")\n",
    "\n",
    "def connect_maria_db():\n",
    "    conn = pymysql.connect(host=MARIA_IP, user=MARIA_USER, password=MARIA_PASSWORD, db=MARIA_NAME, charset='utf8')\n",
    "    return conn\n",
    "\n",
    "def execute_query(query):\n",
    "    conn = connect_maria_db()\n",
    "    df = pd.read_sql(query, conn)\n",
    "    return df\n",
    "\n",
    "query = \"\"\"SELECT content FROM press_content\"\"\"\n",
    "df = execute_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'table', 'pdf', 'txt', 'image'}\n"
     ]
    }
   ],
   "source": [
    "# Check type\n",
    "import json\n",
    "\n",
    "types = set()\n",
    "for i in range(len(df)):\n",
    "    eles = df.iloc[i]['content']\n",
    "    try:\n",
    "        ele_list = json.loads(eles)\n",
    "    except:\n",
    "        continue\n",
    "    for ele in ele_list:\n",
    "        types.add(ele['type'])\n",
    "        \n",
    "print(types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "- Text 추출\n",
    "- Sentence 분리\n",
    "- 특수 문자 제거\n",
    "- 5음절 이상 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Length of Documents : 3983\n",
      "Min Length of Documents : 0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def extract_text(element):\n",
    "    data_list = json.loads(element, strict=False)\n",
    "    output_text = \"\"\n",
    "    for data in data_list:\n",
    "        if data['type'] in (\"image\", \"pdf\", \"table\"): continue\n",
    "        output_text += data['data']\n",
    "    return output_text\n",
    "\n",
    "def preprocess(element):\n",
    "    \n",
    "    # 특수 문자 제거 (이걸 제거하면 특수문자를 생성할 수 없음 있는 그대로 사용하기.)\n",
    "    # element = re.sub(\"[^\\w\\s.,\\(\\)]+|_+\", \" \", element).strip()\n",
    "    # 2개 이상 공간 제거 및 문장 내 newline 제거 (\\n도 제거)\n",
    "    element = re.sub(\"[\\s]+\", \" \", element).strip()\n",
    "    return element\n",
    "\n",
    "def postprocess(elements):\n",
    "    new_elements = []\n",
    "    for element in elements:\n",
    "        # 음절 5개 초과\n",
    "        if 5 >= len(element.split(\" \")): continue\n",
    "        # 특수문자로만 구성된 문장제거\n",
    "        if not re.search('[a-zA-Z0-9가-힇ㄱ-ㅎㅏ-ㅣぁ-ゔァ-ヴー々〆〤一-龥]', element): continue\n",
    "        new_elements.append(element)\n",
    "    return new_elements\n",
    "\n",
    "\n",
    "# Only Text\n",
    "df['content'] = df['content'].map(extract_text)\n",
    "# Preprocess\n",
    "df['content'] = df['content'].map(preprocess)\n",
    "# Split Sentence\n",
    "df['content'] = df['content'].map(sent_tokenize)\n",
    "# Postprocess\n",
    "df['content'] = df['content'].map(postprocess)\n",
    "\n",
    "\n",
    "max_seq_len, min_seq_len = 0, float(\"inf\")\n",
    "for i in range(len(df)):\n",
    "    max_seq_len = max(max_seq_len, len(df.iloc[i]['content']))\n",
    "    min_seq_len = min(min_seq_len, len(df.iloc[i]['content']))\n",
    "print(\"Max Length of Documents :\", max_seq_len)\n",
    "print(\"Min Length of Documents :\", min_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before :  632515\n",
      "After :  629285\n"
     ]
    }
   ],
   "source": [
    "# Drop empty\n",
    "print(\"Before : \", len(df))\n",
    "df.loc[df['content'].str.len() == 0] = pd.NA\n",
    "df = df.dropna()\n",
    "print(\"After : \", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가장 긴 문장 :  158323\n",
      "가장 짧은 문장 :  11\n"
     ]
    }
   ],
   "source": [
    "# 가장 긴  문장\n",
    "max_len, min_len = 0, float(\"inf\")\n",
    "flag = False\n",
    "for i in range(len(df)):\n",
    "    for sen in df.iloc[i]['content']:\n",
    "        max_len = max(max_len, len(sen))\n",
    "        min_len = min(min_len, len(sen))\n",
    "\n",
    "print(\"가장 긴 문장 : \", max_len)\n",
    "print(\"가장 짧은 문장 : \", min_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Train Data for Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../tokenizer/train.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(len(df)):\n",
    "        sentence_list = df.iloc[i]['content']\n",
    "        for sentence in sentence_list:\n",
    "            sentence = sentence.strip()\n",
    "            f.write(sentence)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Pretrain-Data for BERT\n",
    "\n",
    "### prerequisite\n",
    "\n",
    "- Train Tokenizer :  `python train_tokenizer.py` (Take about 10 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents :  629285\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Documents : \", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size :  32000\n",
      "Special tokens {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"../tokenizer\")\n",
    "\n",
    "print(\"Vocab size : \", tokenizer.vocab_size)\n",
    "print(\"Special tokens\", tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train data : 566356\n",
      "Number of valid data : 62929\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, valid_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"Number of train data : {len(train_df)}\")\n",
    "print(f\"Number of valid data : {len(valid_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "masking: 100%|██████████| 566356/566356 [11:50<00:00, 797.09it/s]\n",
      "masking: 100%|██████████| 566356/566356 [11:52<00:00, 794.77it/s]\n",
      "masking: 100%|██████████| 566356/566356 [11:51<00:00, 795.47it/s]\n",
      "masking: 100%|██████████| 566356/566356 [11:48<00:00, 799.84it/s]\n",
      "masking: 100%|██████████| 566356/566356 [11:51<00:00, 796.50it/s]\n",
      "masking: 100%|██████████| 566356/566356 [11:50<00:00, 797.00it/s]\n",
      "masking: 100%|██████████| 566356/566356 [11:48<00:00, 799.55it/s]\n",
      "masking: 100%|██████████| 566356/566356 [11:49<00:00, 798.40it/s]\n",
      "masking: 100%|██████████| 566356/566356 [11:50<00:00, 797.04it/s]\n",
      "masking: 100%|██████████| 566356/566356 [11:51<00:00, 796.07it/s]\n",
      "masking: 100%|██████████| 62929/62929 [01:19<00:00, 793.72it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import tqdm\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "max_len = 512\n",
    "mlm_prob = 0.15 # -> 80% : [MASK], 10% : random word, 10% : keep same\n",
    "\n",
    "def create_pretrain_instances(docs, doc_idx, doc, max_len, mlm_prob, vocab_list):\n",
    "    # CLS tokens SEP tokens SEP\n",
    "    max_seq = max_len - 3\n",
    "\n",
    "    instances = []\n",
    "    cur_chunk = []\n",
    "    cur_length = 0\n",
    "    \n",
    "    for i in range(len(doc)):\n",
    "        cur_chunk.append(doc[i])\n",
    "        cur_length += len(doc[i])\n",
    "\n",
    "        if i == len(doc)-1 or cur_length >= max_seq:\n",
    "            if 0 < len(cur_chunk):\n",
    "                a_end = 1\n",
    "                if 1 < len(cur_chunk):\n",
    "                    a_end = random.randrange(1, len(cur_chunk))\n",
    "                \n",
    "                tokens_a = []\n",
    "                for j in range(a_end):\n",
    "                    tokens_a.extend(cur_chunk[j])\n",
    "                \n",
    "                tokens_b = []\n",
    "                if len(cur_chunk) == 1 or random.random() < 0.5:\n",
    "                    is_next = 0\n",
    "                    random_doc_idx = doc_idx\n",
    "\n",
    "                    while doc_idx == random_doc_idx:\n",
    "                        random_doc_idx = random.randrange(0, len(docs))\n",
    "                    \n",
    "                    random_doc = docs.iloc[random_doc_idx]['content']\n",
    "\n",
    "                    random_start = random.randrange(0, len(random_doc))\n",
    "                    for j in range(random_start, len(random_doc)):\n",
    "                        tokens_b.extend(random_doc[j])\n",
    "\n",
    "                else:\n",
    "                    is_next = 1\n",
    "                    for j in range(a_end, len(cur_chunk)):\n",
    "                        tokens_b.extend(cur_chunk[j])\n",
    "\n",
    "                trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "                assert 0 < len(tokens_b)\n",
    "                assert 0 < len(tokens_b)\n",
    "\n",
    "                tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "                segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "                tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens)-3) * mlm_prob), vocab_list)\n",
    "                instance = {\n",
    "                    \"input_tokens\": tokens,\n",
    "                    \"segment\": segment,\n",
    "                    \"is_next\": is_next,\n",
    "                    \"mask_idx\": mask_idx,\n",
    "                    \"mask_label\": mask_label\n",
    "                }\n",
    "                instances.append(instance)\n",
    "            cur_chunk = []\n",
    "            cur_length = 0\n",
    "\n",
    "    return instances\n",
    "\n",
    "\n",
    "def trim_tokens(tokens_a, tokens_b, max_seq):\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_seq:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            del tokens_a[0]\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "\n",
    "\n",
    "def create_pretrain_mask(tokens, mask_cnt, vocab_list):\n",
    "    cand_idx = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\": continue\n",
    "\n",
    "        if 0 < len(cand_idx) and token.startswith(\"##\"):\n",
    "            cand_idx[-1].append(i)\n",
    "        \n",
    "        else:\n",
    "            cand_idx.append([i])\n",
    "    \n",
    "    random.shuffle(cand_idx)\n",
    "\n",
    "    mask_lms = []\n",
    "    for index_set in cand_idx:\n",
    "        if len(mask_lms) >= mask_cnt: break\n",
    "        if len(mask_lms) + len(index_set) > mask_cnt: continue\n",
    "\n",
    "        for index in index_set:\n",
    "            masked_token = None\n",
    "            if random.random() < 0.8:\n",
    "                masked_token = \"[MASK]\"\n",
    "            else:\n",
    "                if random.random() < 0.5:\n",
    "                    masked_token = tokens[index]\n",
    "                else:\n",
    "                    masked_token = random.choice(vocab_list)\n",
    "            \n",
    "            mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "            tokens[index] = masked_token\n",
    "    \n",
    "    mask_lms = sorted(mask_lms, key=lambda x:x[\"index\"])\n",
    "    mask_idx = [p[\"index\"] for p in mask_lms]\n",
    "    mask_label = [p[\"label\"] for p in mask_lms]\n",
    "\n",
    "    return tokens, mask_idx, mask_label\n",
    "\n",
    "\n",
    "def main(tokenizer, df, output_path, max_len, mlm_prob, count=10):\n",
    "    vocab_list = list(tokenizer.vocab)\n",
    "    vocab_list.remove(\"[UNK]\")\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        sentences = df.iloc[i]['content']\n",
    "        tokenized_sentences = []\n",
    "        for sentence in sentences:\n",
    "            tokenized_sentences.append(tokenizer.tokenize(sentence))\n",
    "        df.iloc[i]['content'] = tokenized_sentences\n",
    "    \n",
    "    for index in range(count):\n",
    "        output = output_path.format(index)\n",
    "        with open(output, 'w') as out_file:\n",
    "            with tqdm.tqdm(total=len(df), desc=\"masking\") as pbar:\n",
    "                for i in range(len(df)):\n",
    "                    doc = df.iloc[i]['content']\n",
    "                    instances = create_pretrain_instances(\n",
    "                                    df,\n",
    "                                    i,\n",
    "                                    doc,\n",
    "                                    max_len,\n",
    "                                    mlm_prob,\n",
    "                                    vocab_list\n",
    "                                )\n",
    "                    \n",
    "                    for instance in instances:\n",
    "                        out_file.write(json.dumps(instance, ensure_ascii=False))\n",
    "                        out_file.write('\\n')\n",
    "                    pbar.update(1)\n",
    "\n",
    "train_output_path = \"pretrain_data_{}.json\"\n",
    "valid_output_path = \"valid_data_{}.json\"\n",
    "main(tokenizer, train_df, train_output_path, max_len, mlm_prob)\n",
    "main(tokenizer, valid_df, valid_output_path, max_len, mlm_prob, count=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyunsoo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
